{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smjune/ipynb/blob/main/1_Llama_index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Wf5bmyZ9sX"
      },
      "source": [
        "# üìì Llama-Index Practice\n",
        "\n",
        "In this section, we will create a simple Llama Index app and learn how to log and get feedback on an LLM response. Additionally, by evaluating our Llama Index app using TruLens, we will see how TruLens assesses an LLM app.  \n",
        "\n",
        "Through this process, we hope to gain a clear understanding of the role and functionalities of the Llama Index, as well as the specific inputs and outputs of each method.  \n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*CrywD0tloiK9dgy_.png\" width=\"600\">\n",
        "\n",
        "This section is divided into three main stages:\n",
        "\n",
        "###I. Build a Query Engine  \n",
        "###II. Use Database Management Method  \n",
        "###III. Initialize Evaluation Metrics and Evaluate Query Engine  \n",
        "\n",
        "Let's first look at what is needed to build a Query Engine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MxLx7YMZ9sa"
      },
      "source": [
        "## I. Buliding a Query Engine\n",
        "\n",
        "The task of this section is to create a Query Engine that takes a user's query, retrieves for related content, and returns a final summary.  \n",
        "\n",
        "Ignore the `Vector DB` in the picture. We don't use external vector database in the pracetice. Instead, we will use local memory as simple vector store.\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/bR4xaBd.png\">\n",
        "\n",
        "To accomplish this task, the following steps will be taken.\n",
        "<br/>\n",
        "\n",
        "#### 1. Install and Import Libraries\n",
        "#### 2. DownLoad Data\n",
        "#### 3. Make Query Engine from Index Directly\n",
        "#### 4. Conduct Activity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Install and Import Libraries\n",
        "\n",
        "```Python\n",
        "! pip install trulens_eval llama_index openai --quiet\n",
        "! pip install packaging==23.2 streamlit==1.35.0 --quiet\n",
        "```\n",
        "```Python\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" #Insert your openai api key\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document\n",
        "from openai import OpenAI\n",
        "\n",
        "from trulens_eval import Tru\n",
        "from trulens_eval.feedback.provider import OpenAI\n",
        "from trulens_eval import Feedback\n",
        "from trulens_eval.app import App\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import textwrap\n",
        "import openai\n",
        "```"
      ],
      "metadata": {
        "id": "yrx4kNH8nPSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "Sjy4Ic6znhcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "gD44b3YRnlRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check if the ‚Ç©OPENAI_API_KEY‚Ç© is properly set as an environment variable by using the following command.\n",
        "\n",
        "```Python\n",
        "! echo $OPENAI_API_KEY\n",
        "```"
      ],
      "metadata": {
        "id": "um2Ynqld5mIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "0w6z6sud2nuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. DownLoad Data\n",
        "\n",
        "Previously, we learned that the Query Engine receives the following elements as input and output.\n",
        "<br/>\n",
        "#### Input: **User Query**, **Vector Database**\n",
        "#### Output: **Summary of Retrieved Passages**\n",
        "<br/>\n",
        "\n",
        "Here, the User Query is an element that we can decide. However, since the Vector Database must contain information related to the Query, we need to consider how to configure the Vector Database before proceeding with the practice.\n",
        "\n",
        "The Vector Database can be divided into two categories: a storage that allows the query engine to retrieve related passages, and the data containing the related passages. In this practice, we will use local memory as a data storage instead of an external Vector Database, so the Vector Database will be referred to as a Vector Store from now on. For the data, we will use a simple txt file provided by Llama Index.\n",
        "\n",
        "Before we begin the practical exercises, let's first take a look at the txt file we used. The code below can be used to download the txt file we used with the `wget` command.\n",
        "\n",
        "```Python\n",
        "!wget https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt -P data/\n",
        "```"
      ],
      "metadata": {
        "id": "rcPNw3hll6Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "kDZwf80Yn0Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there are no issues with the download, you should be able to see that the `paul_graham_essay.txt` file has been downloaded.\n",
        "\n",
        "This example uses the text of Paul Graham‚Äôs essay, [‚ÄúWhat I Worked On‚Äù](https://paulgraham.com/worked.html), and is the canonical llama-index example.\n",
        "\n",
        "\n",
        "Let‚Äôs print the first fifteen lines of the txt file. This will allow us to see that the text file deals with the experiences and reflections of one person's life.\n",
        "\n",
        "\n",
        "```Python\n",
        "path_to_txt = '/path/to/paul_graham_essay.txt' #change this path\n",
        "\n",
        "with open(path_to_txt, 'r', encoding='utf-8') as file:\n",
        "    for i in range(15):\n",
        "        line = file.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        print(line.strip())\n",
        "print(\"\\n...\")\n",
        "```"
      ],
      "metadata": {
        "id": "c_xPguxFn2Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pwd"
      ],
      "metadata": {
        "id": "L5RaLfejg_2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "sw7R2KdSpK4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3W_CgXgZ9se"
      },
      "source": [
        "### 3. Make Query Engine from Index Directly\n",
        "\n",
        "Now let's create a query engine unsing the above information.\n",
        "\n",
        "First, since Llama Index cannot directly handle text files in their raw form, we must convert them into Documents and Nodes. This can be accomplished with the following code.\n",
        "\n",
        "```Python\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "```\n",
        "\n",
        "SimpleDirectoryReader takes the directory path containing data that will go into the Vector Store as input and converts all the files within that directory into Document objects. The data types that can be converted into Documents are as follows.\n",
        "\n",
        "\n",
        "*   .txt - text file  \n",
        "*   .csv - comma-separated values  \n",
        "*   .docx - Microsoft Word  \n",
        "*   .epub - EPUB ebook format  \n",
        "*   .hwp - Hangul Word Processor  \n",
        "*   .ipynb - Jupyter Notebook  \n",
        "*   .jpeg, .jpg - JPEG image  \n",
        "*   .mbox - MBOX email archive  \n",
        "*   .md - Markdown  \n",
        "*   .mp3, .mp4 - audio and video  \n",
        "*   .pdf - Portable Document Format  \n",
        "*   .png - Portable Network Graphics  \n",
        "*   .ppt, .pptm, .pptx - Microsoft PowerPoint  \n",
        "\n",
        "If you want to know your current path, you can use the following command.\n",
        "```Python\n",
        "! pwd\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "bCiKhRTsqxBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can view the text contained in the `documents` using the following command:\n",
        "\n",
        "```Python\n",
        "for i in range(len(documents)):\n",
        "  print(documents[i].text)\n",
        "```"
      ],
      "metadata": {
        "id": "nPADzZXZliSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "CYNRCtUJ4DPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the Document is complete, we need to use this data to create an index.\n",
        "\n",
        "The index is created using the VectorStoreIndex.from_documents() method. This method takes a list of Node objects or a Document as input.\n",
        "\n",
        "\n",
        "```Python\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "```\n",
        "\n",
        "If connected to an external vector database, Llama Index follows the index algorithm of that vector database. If there is no connection to an external vector database, it stores the data in the local memory in the form of a dictionary."
      ],
      "metadata": {
        "id": "AqPodM6LqzvK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07Lz2HFOZ9se"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let‚Äôs look at how the index is structured.  \n",
        "\n",
        "Our index is built based on documents. However, as mentioned in the previous presentation, when Llamaindex stores data in the database, it breaks down the document into nodes. Therefore, let‚Äôs first check if the document has been divided into nodes.\n",
        "\n",
        "```Python\n",
        "node_id = index.index_struct.nodes_dict\n",
        "\n",
        "for key, value in node_id.items():\n",
        "    print(value)\n",
        "    node_example_id = value\n",
        "    break\n",
        "    \n",
        "print(\"The number of nodes: \", len(node_id.values()))\n",
        "\n",
        "print(node_id.values())\n",
        "```"
      ],
      "metadata": {
        "id": "Zsv7Airahigf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "gRohLgpghjJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By looking at this, we can check the id of the first node and the number of nodes the document has been divided into.\n",
        "\n",
        "Next, let‚Äôs see how each node contains their text and has been transformed into embeddings.\n",
        "\n",
        "```Python\n",
        "index._storage_context.docstore.docs[node_example_id].text\n",
        "```\n",
        "\n",
        "```Python\n",
        "index.vector_store.data.embedding_dict[node_example_id]\n",
        "```"
      ],
      "metadata": {
        "id": "hUxlUQJIhnGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "Sv1SawQYhp03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see here, each node is automatically converted into embeddings, becoming vectors when the index is created.\n",
        "\n",
        "Next, let‚Äôs verify why the total number of nodes is as shown. When splitting a document into nodes, Llamaindex uses a class called SentenceSplitter. This class allows us to divide the text data into predefined lengths.\n",
        "\n",
        "```Python\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
        "#you can change chunk_size, chunk_overlap\n",
        "\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "print(len(nodes))\n",
        "```"
      ],
      "metadata": {
        "id": "iDDAJ7eFhqKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "z3dPaEmDl65i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each node has an attribute called `text`, allowing us to see what text data each node contains. Additionally, you can see that the texts overlap by the value of the `chunk_overlap`.\n",
        "\n",
        "When checking the length of each text data, we can see that they are composed of different numbers of characters. This is because the `SentenceSplitter` class divides `documents` not by the number of characters but by tokens, which are a type of sub-word.\n",
        "\n",
        "```Python\n",
        "print(nodes[0].text)\n",
        "print(\"\\n----------------------\\n\")\n",
        "print(nodes[1].text)\n",
        "print(\"\\n----------------------\\n\")\n",
        "print(len(nodes[0].text), len(nodes[1].text))\n",
        "```"
      ],
      "metadata": {
        "id": "yGThUiKwH925"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "lUl607nO4WMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the default tokenizer used by Llamaindex‚Äôs `SentenceSplitter` to directly check how many tokens each text consists of.\n",
        "\n",
        "By running the code below, we can bring in the tokenizer of `gpt-3.5-turbo`, which is used by default in `SentenceSplitter`, and perform tokenization to see the results and check how many tokens are generated.\n",
        "\n",
        "When we look at the tokenization results, we will see that they differ from the `chunk_size` we set. This is because, when creating nodes, the amount of text data to be tokenized is determined by considering the metadata as well, in order to compose nodes of consistent size including the amount of metadata.\n",
        "\n",
        "```Python\n",
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "encoded_text = enc.encode(nodes[0].text)\n",
        "print(f\"Encoded text: {encoded_text}\")\n",
        "print(f\"Encoded text length: {len(encoded_text)}\")\n",
        "\n",
        "decoded_text = enc.decode(encoded_text)\n",
        "print(f\"Decoded text: \\n\\ndecoded_text}\")\n",
        "```"
      ],
      "metadata": {
        "id": "BacVcf_pASJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "Q-kbIoe2A2Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the result here matches the number we saw earlier.  \n",
        "\n",
        "If you want to change the default `chunk_size` or `chunk_overlap` when you make `index`, you can use `transformations` argument in `VectorStoreIndex.from_documents()`.\n",
        "\n",
        "```Python\n",
        "text_splitter = SentenceSplitter(chunk_size=200, chunk_overlap=50)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents=documents, transformations=[text_splitter])\n",
        "```\n",
        "```Python\n",
        "node_id = index.index_struct.nodes_dict\n",
        "\n",
        "for key, value in node_id.items():\n",
        "    print(key, value)\n",
        "    node_example_id = value\n",
        "    break\n",
        "\n",
        "print(\"The number of nodes: \", len(node_id.values()))\n",
        "\n",
        "print(node_id.values())\n",
        "```"
      ],
      "metadata": {
        "id": "oyGybnKu-or3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "cbjeu28V-pTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "MkVokW7HNBOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But, in this practice, we will use default `SentenceSplitter`.\n",
        "\n",
        "```Python\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "```"
      ],
      "metadata": {
        "id": "Jn8Asvmq-2Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "ha1vQ7WK_Hgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the index is created, Llama Index provides a way to immediately create a query engine using the index. With the following code, we can create a query engine using the index.\n",
        "\n",
        "```Python\n",
        "query_engine = index.as_query_engine()\n",
        "```"
      ],
      "metadata": {
        "id": "891BppEPtHGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "053GSAHpu0_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Conduct Activity\n",
        "\n",
        "Now let's actually use the query engine. We will check the results of query engine using the following questions.\n",
        "\n",
        "#### 4-1. Answerable Questions from Data\n",
        "\n",
        "In this activity, we will check how query engine will answer **a question related to the given data, and the information needed for the correct answer is directly provided in the data**.\n",
        "\n",
        "#### 4-2. Unrelated Question to the Data\n",
        "\n",
        "In this activity, we will check how query engine will answer **a question that has absolutely nothing to do with the given data**.\n",
        "\n",
        "#### 4-3. Question Requiring Complex Reasoning\n",
        "\n",
        "In this activity, we will check how query engine will answer **a question that is hard to answer, which means that question is related to the given data, but the information needed for the correct answer is not directly given in the data**."
      ],
      "metadata": {
        "id": "F-7wakUBE1Sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-1. Answerable Questions from Data\n",
        "First, let's ask the query engine some questions that it can answer and some that it cannot.\n",
        "\n",
        "For example, we could ask what program the author of this data has written about. The answer to this question is in the second sentence of the txt file.\n",
        "\n",
        "#### Question: **What is the first programs the author tried writing?**\n",
        "\n",
        "#### In paul_graham_essay: **The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\"**\n",
        "\n",
        "Intuitively, it is expected that the query engine will generate the correct answer, which is IBM 1401. Let's verify that.\n",
        "\n",
        "```Python\n",
        "text_chunk = \"The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \\\"data processing.\\\"\"\n",
        "\n",
        "if text_chunk in documents[0].text:\n",
        "    print(\"True\")\n",
        "else:\n",
        "    print(\"False\")\n",
        "```\n",
        "\n",
        "```Python\n",
        "response = query_engine.query(\"What is the first programs the author tried writing?\")\n",
        "\n",
        "print(response)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "Bmc-gIJTWtNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "JrEFNqXHBZre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3509db49-8007-4fed-9ba1-46aba6551fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "xKexKfgIWHbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d3c445-a43b-43e6-d76f-142c693ae517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first programs the author tried writing were on the IBM 1401 using an early version of Fortran.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://xe.obg.co.kr/files/attach/images/4199/352/004/7e2189cf45a27f9b9cda5fef28c1dd5f.gif\">\n",
        "\n",
        "As expected, the results are accurate.  \n"
      ],
      "metadata": {
        "id": "ThOFtaBUL42B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-2. Unrelated Question to the Data\n",
        "\n",
        "This time, We will check whether the query engine performs its intended function well.  \n",
        "A query engine is **a generic interface that allows users to ask questions about external data**.  \n",
        "Therefore, **it should not be able to answer questions that cannot be answered through external data**.\n",
        "\n",
        "The reason is that if the query engine can answer questions that are completely unrelated to the external data, it implies that the query engine can freely use parameterized knowledge. This, in turn, means that the answers from the query engine may contain information that is incorrect or not up-to-date.\n",
        "\n",
        "<img src=\"https://media0.giphy.com/media/ANbD1CCdA3iI8/200w.gif?cid=6c09b952ooe7fzryithl047ty5npdt1xd50tlu9gcpqnzh87&ep=v1_gifs_search&rid=200w.gif&ct=g\" width=\"200\" height=\"200\"><img src=\"https://st2.depositphotos.com/4421345/11492/v/950/depositphotos_114921728-stock-illustration-public-speaking-robot.jpg\" width=\"200\">\n",
        "\n",
        "So, let‚Äôs pass on a question that has nothing to do with the original data but that the query engine‚Äôs LLM can answer.  \n",
        "For example, we could use a question like this.\n",
        "\n",
        "#### Question: **How many countries participated in the production of the space station?**\n",
        "#### Answer: **15**\n",
        "\n",
        "```Python\n",
        "response = query_engine.query(\"How many countries participated in the production of the space station?\")\n",
        "\n",
        "print(response)\n",
        "```"
      ],
      "metadata": {
        "id": "3TDA0WrJyrJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "ChGlKMz8zzzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://img.danawa.com/images/descFiles/6/164/5163558_1_16652362092043267.gif\n",
        "\" width = 200>\n",
        "\n",
        "It doesn't produce the correct answer as expected. However, we can find what kinds of LLM used in query engine through the official LlamaIndex documentation.\n",
        "\n",
        "Actually, Llamaindex uses `gpt-3.5-turbo` as LLM of response synthesizer. So, we can ask `gpt-3.5-turbo` to answer the question.\n",
        "\n",
        "Let's check this out with a simple code. We can use the following code to separately utilize the OpenAI LLM.  \n",
        "\n",
        "\n",
        "```Python\n",
        "def generate_answer(question):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": question,\n",
        "        },\n",
        "    ]\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        temperature=0,\n",
        "        messages=messages,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "```\n",
        "```Python\n",
        "question = \"How many countries participated in the production of the space station?\"\n",
        "\n",
        "print(generate_answer(question))\n",
        "```"
      ],
      "metadata": {
        "id": "aIge8Ztb0O5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "BZc57zER00Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "Msc4R2rf1rNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact, as in the example above, OpenAI LLM already knows the answer to this question.  \n",
        "However, since the relevant information cannot be found in the txt file that query engine can refer to here, it does not generate a correct answer for a question that is already known.  \n",
        "\n",
        "<img src=\"https://i.imgur.com/sxSRQks.png\">\n",
        "\n",
        "From this, we can understand that LLM in the response synthesizer of the query engine uses the information from the retrieved passages produced by the retriever, excluding its own parametrized knowledge.  "
      ],
      "metadata": {
        "id": "XUeXErcL1xJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-3. Question Requiring Complex Reasoning\n",
        "\n",
        "Now, let‚Äôs explore more deeply. There is a question related to the data that is hard to answer. We will see how the query engine responds to such questions.\n",
        "\n",
        "The question we will ask in this section is who is the author of the `paul_graham_essay.txt`. Becuase the original text file does not mention the author's name directly, this is a complex question.\n",
        "\n",
        "Question: **Who is the author?**\n",
        "\n",
        "Let's see whether the writer's name is actually not in the original text file or not.\n",
        "\n",
        "```Python\n",
        "path_to_txt = '/path/to/paul_graham_essay.txt'\n",
        "wrapper = textwrap.TextWrapper(width=80)\n",
        "\n",
        "with open(path_to_txt, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        if 'paul graham' in line.lower():\n",
        "            formatted_text = wrapper.fill(line.strip())\n",
        "            print(formatted_text.split('.')[-1])\n",
        "```"
      ],
      "metadata": {
        "id": "yWsZtOwxwRIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "niJUV9XiWqkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the only sentence in the original txt file that mentions 'Paul Graham'.  \n",
        "Now, let's read it ourselves and consider whether it actually specifies the author's name.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Someone might infer that the author's name is Paul Graham from this sentence, but I think it's not certain\n",
        "\n",
        "In my opinion, this sentence does not explicitly state the author's name, so it does not provide enough useful information\n",
        "\n",
        "<br/>\n",
        "\n",
        "However, opinions on this may vary, so let's verify it ourselves.\n",
        "\n",
        "```Python\n",
        "response_complex = query_engine.query(\"Who is the author?\")\n",
        "\n",
        "print(response_complex)\n",
        "```"
      ],
      "metadata": {
        "id": "GhtBI4jBWpT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "q7wyN7zVZPiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i3.ruliweb.net/ori/21/07/30/17af799f631524cc2.gif\">\n",
        "\n",
        "Hmm... The query engine answers better than we think.  \n",
        "Then, let's see what data were referred to by the query engine.\n",
        "<br/>\n",
        "\n",
        "<img src=\"https://i.imgur.com/sxSRQks.png\">\n",
        "\n",
        "The query engine made using `as_query_engine` uses a retriever generated directly from the index by using `as_retriever` method.   \n",
        "Therefore, if we look at the results of this retriever, we can see which retrieved passages the query engine has seen.\n",
        "\n",
        "```Python\n",
        "retriever = index.as_retriever()\n",
        "\n",
        "ret_passages = retriever.retrieve(\"Who is the author?\")\n",
        "\n",
        "for i in range(len(ret_passages)):\n",
        "  print(\"###Retrieved Passage\\n\", ret_passages[i].text)\n",
        "  print(\"\\n\\n\\n\")\n",
        "```"
      ],
      "metadata": {
        "id": "Ho2a0VGdZRWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "jLDyxwvSbZEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is quiet lone passages. So, it is hard to check whether there is any evidence of query engine's response or not.  \n",
        "Can you find any supporting evidence of the question?  \n",
        "\n",
        "Let's see if 'Paul Graham' is in there.\n",
        "\n",
        "```Python\n",
        "passages = ''\n",
        "\n",
        "for i in range(len(ret_passages)):\n",
        "  passages += ret_passages[i].text\n",
        "\n",
        "if 'paul graham' in passages.lower():\n",
        "    print(\"The word 'Paul Graham' is found in the passages.\")\n",
        "else:\n",
        "    print(\"The word 'Paul Graham' is not found in the passages.\")\n",
        "```"
      ],
      "metadata": {
        "id": "_2FW06focNnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "pfd8-X0VckxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i2.ruliweb.net/ori/21/07/24/17ad8e756ba54811a.gif\" width=\"400\">\n",
        "\n",
        "???\n",
        "\n",
        "Retriever couldn't find the only sentence with the word Paul Graham on it. Then how did query engine know that the answer was Paul Graham?\n",
        "\n",
        "To verify this, let‚Äôs using the query engine‚Äôs LLM as we did in the previous section and ask the same question. We will use a prompt as similar as possible to the one used in LlamaIndex response synthesizer, but with an additional instruction to provide the reason for the LLM‚Äôs answer.  \n",
        "\n",
        "The prompt used by the original Response Synthesizer is as follows.\n",
        "\n",
        "\n",
        "<img src = \"https://i.imgur.com/ZXUDAJi.jpeg\" width=\"450\">  \n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "Therefore, we will write the code as shown below to check the results of the LLM.\n",
        "\n",
        "\n",
        "```Python\n",
        "ret_context = \"\"\n",
        "for ret_result in ret_passages:\n",
        "  ret_context += ret_result.text\n",
        "\n",
        "question = f\"\"\"Context information is below.\n",
        "---------------------\n",
        "{ret_context}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge,\n",
        "answer the query and the reasons of answer.\n",
        "Query: Who is the author?\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "print(generate_answer(question))\n",
        "```\n"
      ],
      "metadata": {
        "id": "8sX-CgLw8RrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "6kZ5mq-L0ZPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well... it seems like query engine can **capture useful information not only in direct evidence but also in indirect content by using its prior knowledge, i.e., parametrized knowledge**.  \n",
        "\n",
        "Therefore, we can consider that the query engine has the ability to provide correct answers even when the given query is not explicitly answered in the data.  \n",
        "\n",
        "However, the problem is that **without such an explanation, the reasoning process cannot be trusted** because it is impossible to know whether the LLM used reliable information in its reasoning.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Here‚Äôs what we can infer from this:\n",
        "\n",
        "the query engine has the capability to answer not only questions directly addressed in the data **but also questions that can be indirectly inferred from the data.**  \n",
        "However, in such cases, **the reliability of the answers is compromised because we cannot verify the information used and the reasoning steps.**   \n",
        "\n",
        "To address this issue, we can use one of the following two methods.  \n",
        "\n",
        "\n",
        "#### **1.** Force the response synthesizer to utilize only the information in a given retrieved passages.\n",
        "#### **2.** When the response synthesizer generates a summary, make sure that the reason is also generated."
      ],
      "metadata": {
        "id": "TYYM-hUmSFnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. Use Database Management Method\n",
        "\n",
        "Now, let‚Äôs consider a scenario. If we have new data and want to create a query engine using it, we could recreate the `document`, `index`, and `query engine` as we did above.\n",
        "\n",
        "However, this is quite cumbersome and time-consuming. In addition, if you want to insert new data so that query engine uses that data, the above method is very expansive because a new index must be generated for every inserts operation.\n",
        "\n",
        "So, Llama Index is providing the following functions to manage data smoothly."
      ],
      "metadata": {
        "id": "U8r7tRdutIFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Insert\n",
        "\n",
        "We can insert new data to the existing query engine.\n",
        "\n",
        "To check this, we will ask a question which can't get useful information from the data before inserting new data, and see what happens after inserting data.\n",
        "\n",
        "In this section, the following data and question will be used.\n",
        "\n",
        "Inserted Data: **\"Natural diamonds were (and are) formed (thousands of million years ago) in the upper mantle of Earth in metallic melts at temperatures of 900‚Äì1,400‚Äâ¬∞C and at pressures of 5‚Äì6‚ÄâGPa.\"**  \n",
        "Question: **What are the temperature and air pressure conditions under which natural diamonds are produced?**\n",
        "\n",
        "```Python\n",
        "data_text = \"Natural diamonds were (and are) formed (thousands of million years ago) in the upper mantle of Earth in metallic melts at temperatures of 900‚Äì1,400‚Äâ¬∞C and at pressures of 5‚Äì6‚ÄâGPa.\"\n",
        "question = \"What are the temperature and air pressure conditions under which natural diamonds are produced?\"\n",
        "\n",
        "res = query_engine.query(question)\n",
        "\n",
        "print(res)\n",
        "```"
      ],
      "metadata": {
        "id": "iTcP18oNvMQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "eTI0cUhkwOLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://i.pinimg.com/originals/15/8b/ed/158bed9819e4fccf7e18a5eeeaf79c6b.png\" width = 200>\n",
        "\n",
        "Hmm‚Ä¶ the query engine answers quite well. It seems the response synthesizer is using its parameterized knowledge. However, there are no specific details like 900‚Äì1,400¬∞C and pressures of 5‚Äì6 GPa.\n",
        "\n",
        "Anyway, it's time to insert the data.\n",
        "\n",
        "```Python\n",
        "docu = Document(text=data_text, id_=\"new_doc_id\")\n",
        "\n",
        "index.insert(docu)\n",
        "\n",
        "new_query_engine = index.as_query_engine()\n",
        "```\n",
        "\n",
        "```Python\n",
        "new_res = new_query_engine.query(question)\n",
        "\n",
        "print(new_res)\n",
        "```"
      ],
      "metadata": {
        "id": "2PBUYCIQw0Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "iVEMtfqDXcuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "0dWkyfE_yXWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's as expected. Query engine is generating an appropriate answer with detail using information in new `Document` object.\n",
        "\n",
        "Then, what happens if we change the information in that data?"
      ],
      "metadata": {
        "id": "FCLimkaAyouz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Update\n",
        "\n",
        "If a Document is already present within an index, you can \"update\" a `Document` with the same doc `id_`. The `update_ref_doc` method receives a single `Document` object and updates the value of data that has the same `id`.\n",
        "\n",
        "Or you can \"refresh\" all document at once. The `refresh_ref_doc` method finds the `id` of the `Document` object that came into the input, updates the data with the same `id`, or inserts the data if there is no other data with the same `id`\n",
        "\n",
        "**`update_ref_doc`**  \n",
        "**Input**: single `Document` object  \n",
        "**Output**: None\n",
        "\n",
        "**`refresh_ref_doc`**  \n",
        "**Inpu**t: list of `Document` object  \n",
        "**Output**: a boolean list, indicating which documents in the input have been refreshed in the index.\n",
        "\n",
        "You will see `False` in that boolean list if text of document doesn't change.  \n",
        "\n",
        "In both methods, we can set `delete_from_docstore` to `True` or `False`. A detailed description of this will be given in Delete.\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "Let's check by changing the base question a little bit.  \n",
        "\n",
        "I changed the information about temperature and air pressure in the question, so check it out for yourself.  \n",
        "\n",
        "```Python\n",
        "docu.text = \"Natural diamonds were (and are) formed (thousands of million years ago) in the upper mantle of Earth in metallic melts at temperatures of 2,000‚Äì6,000‚Äâ¬∞C and at pressures of 8‚Äì9‚ÄâGPa.\"\n",
        "\n",
        "output = index.update_ref_doc(\n",
        "    docu,\n",
        "    update_kwargs={\"delete_kwargs\": {\"delete_from_docstore\": True}},\n",
        ")\n",
        "\n",
        "print(output)\n",
        "\n",
        "query_engine_update = index.as_query_engine()\n",
        "\n",
        "res_update = query_engine_update.query(question)\n",
        "\n",
        "print(res_update)\n",
        "```\n",
        "\n",
        "```Python\n",
        "docu.text = \"Natural diamonds were (and are) formed (thousands of million years ago) in the upper mantle of Earth in metallic melts at temperatures of 6,000‚Äì8,000‚Äâ¬∞C and at pressures of 12‚Äì15‚ÄâGPa.\"\n",
        "\n",
        "output = index.refresh_ref_docs(\n",
        "    [docu]\n",
        ")\n",
        "\n",
        "print(output)\n",
        "\n",
        "query_engine_refresh = index.as_query_engine()\n",
        "\n",
        "res_refresh = query_engine_refresh.query(question)\n",
        "\n",
        "print(res_refresh)\n",
        "```"
      ],
      "metadata": {
        "id": "z44P9aUJ1LP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "f71hHZ4rSSyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "DS3W8U08Uzcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check list of doc_id in the index. So, if you want more experience with other data, you can check whether your data is inserted properly by looking at that list\n",
        "\n",
        "```Python\n",
        "print(index.ref_doc_info.keys())\n",
        "```"
      ],
      "metadata": {
        "id": "vsczfcTQUVNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "UHew38_oARe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Delete\n",
        "\n",
        "We wrapped the new information above with a `Document` object and inserted it into the index.\n",
        "\n",
        "At this time, we were able to specify the `Document` object using `doc_id`. What this doc_id is can be confirmed as follows.\n",
        "\n",
        "```Python\n",
        "id = docu.doc_id\n",
        "\n",
        "print(id)\n",
        "```"
      ],
      "metadata": {
        "id": "N04g56LQzlLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLNzIix5tC8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this, we can change the contents of data with the `doc_id` using the `delete` method of the index.\n",
        "\n",
        "Let's delete the data related to a diamond, and see how query engine answers for the same question.\n",
        "\n",
        "Through the value of `delete_from_docstroe`, it is possible to determine whether the data with that id actually disappears on the database, or disappears only on the index and remains in the database. Even if it disappears only on the index, the information of the data cannot be used by the query engine.\n",
        "\n",
        "\n",
        "```Python\n",
        "index.delete_ref_doc(id, delete_from_docstore=True)\n",
        "\n",
        "query_engine_delete = index.as_query_engine()\n",
        "\n",
        "res_delete = query_engine_delete.query(question)\n",
        "\n",
        "print(res_delete)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "7O3nZyZu0XyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "eKH2oCv_tDb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the answer still contains very details of condition of natural diamonds, check the index whether that data was deleted properly.\n",
        "\n",
        "The result of `index.ref_doc_info.keys()` has to contain only one doc_id.\n",
        "\n",
        "```Python\n",
        "print(index.ref_doc_info.keys())\n",
        "```"
      ],
      "metadata": {
        "id": "sdgmMgJdXOki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "UW-_0dzkXAQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oltar1myZ9se"
      },
      "source": [
        "## III. Evaluate Query Engine\n",
        "\n",
        "In this section, we will define evaluation metrics and go through the process of evaluating the performance of the query engine, in order to become familiar with the TruLens API, which is used to measure the performance of RAG later on.\n",
        "<br/>\n",
        "\n",
        "<img src=\"https://www.trulens.org/assets/images/RAG_Triad.jpg\" width=\"600\">\n",
        "\n",
        "For evaluation, we will leverage the \"hallucination triad\" of groundedness, context relevance and answer relevance.\n",
        "\n",
        "Simply put, using the three metrics described above, we can verify whether the summaries generated by the query engine are supported by the retrieved passages and related to the query. Detailed explanations of the evaluation metric and TruLens will be covered in the subsequent RAG Practice.\n",
        "\n",
        "The purpose of this section is to become familiar with the inputs and outputs used in creating evaluation metrics with TruLens.\n",
        "\n",
        "This section is composed of the following four stages:\n",
        " <br/>\n",
        "#### 1. Initialize Feedback Functions\n",
        "#### 2. Make Instrument app for Logging with TruLens\n",
        "#### 3. Check Records and Feedback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Initialize Feedback Functions\n",
        "\n",
        "In this section, we will define the metrics used for evaluation. The evaluation metric can be implemented using the `Feedback` method of TruLens.\n",
        "\n",
        "```Python\n",
        "tru = Tru()\n",
        "\n",
        "provider = OpenAI()\n",
        "\n",
        "context = App.select_context(query_engine)\n",
        "\n",
        "f_groundedness = (\n",
        "    Feedback(provider.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
        "    .on(context.collect())\n",
        "    .on_output()\n",
        ")\n",
        "\n",
        "f_answer_relevance = (\n",
        "    Feedback(provider.relevance, name = \"Answer Relevance\")\n",
        "    .on_input_output()\n",
        ")\n",
        "\n",
        "f_context_relevance = (\n",
        "    Feedback(provider.context_relevance_with_cot_reasons, name = \"Context Relevance\")\n",
        "    .on_input()\n",
        "    .on(context.collect())\n",
        "    .aggregate(np.mean)\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "9X0FDeNeFQyz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um0swyp_Z9sf"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLMEiSoxZ9sf"
      },
      "source": [
        "### 2. Make Instrument app for Logging with TruLens\n",
        "\n",
        "Next, it is necessary for TruLens to recognize the defined evaluation metric and to apply the query engine to the TruLens API. This can be accomplished with the code below. Here, the evaluation results can be checked using the `app_id`.\n",
        "\n",
        "```Python\n",
        "from trulens_eval import TruLlama\n",
        "tru_query_engine_recorder = TruLlama(query_engine,\n",
        "    app_id='LlamaIndex_App1',\n",
        "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance])\n",
        "```\n",
        "```Python\n",
        "with tru_query_engine_recorder as recording:\n",
        "    query_engine.query(\"What did the author do growing up?\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGjjL17TZ9sf"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZIUi51lZ9sf"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPMgcjBGZ9sf"
      },
      "source": [
        "### 3. Check Records and Feedback\n",
        "\n",
        "Now, let's see which values of the query engine were evaluated by the TruLens API and how the evaluation turned out. First, using the code below, we can view the input, intermediate values, and output of the query engine as recognized by TruLens.\n",
        "\n",
        "```Python\n",
        "eval_record = recording.get()\n",
        "\n",
        "print(\"###Query\\n\", eval_record.main_input)\n",
        "print(\"\"\"      ‚Üì\n",
        "      ‚Üì\n",
        "      ‚Üì\n",
        "      ‚Üì   Retriever retrieved relevant passages.\n",
        "      ‚Üì\n",
        "      ‚Üì\n",
        "      ‚Üì\"\"\")\n",
        "for i in range(len(eval_record.calls[0].rets)):\n",
        "  print(\"###Retrieved Passages \\n\", eval_record.calls[0].rets[i]['node']['text'], \"\\n\\n\")\n",
        "print(\"\"\"      ‚Üì\n",
        "      ‚Üì\n",
        "      ‚Üì\n",
        "      ‚Üì   Query engine summarize retrieved passages.\n",
        "      ‚Üì\n",
        "      ‚Üì\n",
        "      ‚Üì\"\"\")\n",
        "print(\"###Summary of Query Engine \\n\", eval_record.main_output)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_dbRUq0Z9sf"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If it's confirmed that there are no issues with the values of the query engine received by TruLens, now let's check the actual results.\n",
        "```Python\n",
        "tru.run_dashboard()\n",
        "```"
      ],
      "metadata": {
        "id": "ANT7mswZHbZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogFZmUGnZ9sf"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TruLens dashboard can be accessed through an external URL.  \n",
        "\n",
        "The method to check the scores without accessing the dashboard is as follows\n",
        "\n",
        "```Python\n",
        "tru.get_leaderboard(app_ids=[\"LlamaIndex_App1\"])\n",
        "```\n",
        "```Python\n",
        "rec = recording.get()\n",
        "\n",
        "for feedback, feedback_result in rec.wait_for_feedback_results().items():\n",
        "    print(feedback.name, feedback_result.result)\n",
        "```"
      ],
      "metadata": {
        "id": "R-rY2NEGIMlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "cc0wUbiNJzJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TES672H6Z9sf"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "records, feedback = tru.get_records_and_feedback(app_ids=[\"LlamaIndex_App1\"])\n",
        "\n",
        "records.head()\n",
        "```"
      ],
      "metadata": {
        "id": "uWheeBUQIkQY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a72kl3FyZ9sf"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see the evidence for LLM's evaluation.  \n",
        "\n",
        "If you don't change the `app_id` in `recording`, and evaluate query engine again, then you will see one more row in the results of the previous shell.  \n",
        "\n",
        "To see the evidence of that row, you have to change `i` to index of row.\n",
        "\n",
        "```Python\n",
        "i=0 #Index of the evaluation that you want to check\n",
        "\n",
        "print(records['Answer Relevance_calls'][i][0]['ret']) #Answer Relevance score\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"-------------------------------------\")\n",
        "\n",
        "for idx, sentence in enumerate(records['Groundedness_calls'][i][0]['args']['statement'].split('. ')):\n",
        "  print(f\"STATEMENT {idx}\", sentence) #Query Engine answer\n",
        "print(\"\\n\")\n",
        "\n",
        "for j in range(len(records['Groundedness_calls'][i])):\n",
        "  print(records['Groundedness_calls'][i][j]['meta']['reasons']) #Groundedness evidence\n",
        "  print(records['Groundedness_calls'][i][j]['ret']) #Groundedness score\n",
        "  print(\"-------------------------------------\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "for j in range(len(records['Context Relevance_calls'][i])):\n",
        "  print(records['Context Relevance_calls'][i][j]['meta']['reason']) #Context Relevance evidence\n",
        "  print(records['Context Relevance_calls'][i][j]['ret']) #Context Relevance score\n",
        "  print(\"-------------------------------------\")\n",
        "```"
      ],
      "metadata": {
        "id": "8gl0a0-dNFUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "xXKpGN9PH10e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything is okay, then stop the dashboard.\n",
        "\n",
        "```Python\n",
        "tru.stop_dashboard()\n",
        "```"
      ],
      "metadata": {
        "id": "fprdwdPv0aWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "wZozKd300a87"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11.4 ('agents')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "7d153714b979d5e6d08dd8ec90712dd93bff2c9b6c1f0c118169738af3430cd4"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}